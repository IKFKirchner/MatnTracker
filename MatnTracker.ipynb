{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# MatnTracker: Mapping hadith\n",
    "#################################\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import subprocess\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Generate the keygrams and keywords from input file CSV1.\n",
    "\n",
    "# Load your CSV file\n",
    "input_csv_path = 'CSV1/Input/Path/filename.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "original_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Tokenize and create a list of all tokens\n",
    "all_tokens = []\n",
    "for text in original_df['Matn']:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Create a frequency list of all tokens\n",
    "all_tokens_frequency_list = Counter(all_tokens)\n",
    "\n",
    "# Function to find the lowest and second-lowest frequency tokens in a text\n",
    "def find_lowest_tokens(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    token_frequencies = [(token, all_tokens_frequency_list[token]) for token in tokens]\n",
    "    token_frequencies.sort(key=lambda x: x[1])\n",
    "    return token_frequencies[0][0], token_frequencies[1][0]\n",
    "\n",
    "# Apply the function to each row and create the output DataFrame\n",
    "original_df['Lowest Frequency Token'], original_df['Second Lowest Frequency Token'] = zip(*original_df['Matn'].apply(find_lowest_tokens))\n",
    "\n",
    "Keyword = original_df['Second Lowest Frequency Token']\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Tokenize the Arabic text into all 4-grams\n",
    "def get_4grams(text):\n",
    "    text = text.split()  # Split text into words\n",
    "    return [' '.join(text[i:i+4]) for i in range(len(text) - 3)]\n",
    "\n",
    "# Create a DataFrame with Matn_ID, Matn, and 4-grams\n",
    "for i, row in original_df.iterrows():\n",
    "    matn_id = row['Matn_ID']\n",
    "    matn_text = row['Matn']\n",
    "    if not pd.isna(matn_text):  # Check for empty cells\n",
    "        matn_4grams = get_4grams(matn_text)\n",
    "        if not matn_4grams:  # If the text is too short for 4-grams, use the whole text\n",
    "            matn_4grams = [matn_text]\n",
    "        results.append([matn_id, matn_text, matn_4grams])\n",
    "\n",
    "# Create a new DataFrame with the desired columns\n",
    "result_df = pd.DataFrame(results, columns=['Matn_ID', 'Matn', '4-grams'])\n",
    "\n",
    "# Initialize an empty dictionary to store 4-gram frequencies\n",
    "all_4grams_frequencies = {}\n",
    "\n",
    "# Initialize a dictionary to store the highest TFIDF words\n",
    "highest_tfidf_words = {}\n",
    "\n",
    "# Collect all 4-grams into single list\n",
    "all_ngrams = [ngram for sublist in result_df['4-grams'].tolist() for ngram in sublist]\n",
    "\n",
    "# Initialize and fit the TFIDF vectorizer using all collected 4-grams\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(all_ngrams)\n",
    "\n",
    "# Transform 4-grams to get TFIDF scores\n",
    "for index, row in result_df.iterrows():\n",
    "    ngrams = row['4-grams']\n",
    "    if ngrams:\n",
    "        tfidf_matrix = tfidf_vectorizer.transform(ngrams)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.toarray()[0]\n",        
    "        max_score_index = tfidf_scores.argmax()\n",  
    "        highest_tfidf_word = feature_names[max_score_index]\n",
    "        highest_tfidf_words[row['Matn_ID']] = highest_tfidf_word\n",
    "    else:\n",
    "        highest_tfidf_words[row['Matn_ID']] = "ERROR"\n",
    "\n",
    "# Add a new column for 4-gram frequencies\n",
    "result_df['4-gram_Frequencies'] = result_df['4-grams'].apply(lambda x: [all_4grams_frequencies[n] for n in x])\n",
    "\n",
    "# Add a column for \"Keygram\" based on the specified criteria\n",
    "def find_keygram(matn_id, ngrams):\n",
    "    min_frequency = min([all_4grams_frequencies[n] for n in ngrams])\n",
    "    potential_keygrams = [n for n in ngrams if all_4grams_frequencies[n] == min_frequency]\n",
    "    for n in potential_keygrams:\n",
    "        if highest_tfidf_words[matn_id] in n:\n",
    "            return n\n",
    "    return potential_keygrams[0]\n",
    "\n",
    "# Add a column for \"Keygram\" based on the specified criteria\n",
    "result_df['Keygram'] = result_df.apply(lambda x: find_keygram(x['Matn_ID'], x['4-grams']), axis=1)\n",
    "\n",
    "# Generate two output files, OutputFile2 will be the input file for the Ripgrep, outputFile1\n",
    "\n",
    "# Add a column for \"Keyword\" based on the specified criteria\n",
    "result_df['Keyword'] = Keyword\n",
    "\n",
    "# Create outputFile1 with Matn_ID, Matn, 4-grams, 4-grams_Frequencies, Keygram, Keyword for checking\n",
    "# Change file path to your desired path\n",
    "result_df.to_csv('Output1/File/Path/filename.csv', index=False)\n",
    "\n",
    "# Create OutputFile2 with only Matn_ID, Matn, Keygram, and Keyword columns\n",
    "selected_columns = ['Matn_ID', 'Matn', 'Keygram', 'Keyword']\n",
    "result2_df = result_df[selected_columns]\n",
    "# Change file path to your desired path\n",
    "result2_csv_path = 'Output2/File/Path/filename.csv'\n",
    "result2_df.to_csv(result2_csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f379e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ripgrep the Keygrams on txt files in txt_files_directory\n",
    "\n",
    "txt_files_directory = 'Path/To/Your/Target/Texts/Folder'\n",
    "\n",
    "df = pd.read_csv(result2_csv_path, encoding='utf-8')\n",
    "\n",
    "results = []\n",
    "\n",
    "# Create a dictionary to store the value of X for each Matn_ID\n",
    "keywords = {}\n",
    "\n",
    "# Loop through the input file to get the value of X for each Matn_ID\n",
    "for index, row in df.iterrows():\n",
    "    matn_id = row['Matn_ID']\n",
    "    keyword = row['Keyword']\n",
    "    keywords[matn_id] = keyword\n",
    "\n",
    "# Loop through the rows of the input DataFrame and search for the strings in column Keygram\n",
    "for index, row in df.iterrows():\n",
    "    matn_id = row['Matn_ID']\n",
    "    search_string = row['Keygram']\n",
    "    rg_command = f'rg -l --smart-case \"{search_string}\" {txt_files_directory}'\n",
    "    process = subprocess.Popen(rg_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "    if process.returncode == 0:\n",
    "        txt_files_found = stdout.decode('utf-8').strip().split('\\n')\n",
    "        for txt_file in txt_files_found:\n",
    "            with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                for line_number, line in enumerate(lines, start=1):\n",
    "                    if search_string in line:\n",
    "                        # Check if \"X\" is present in the line (4th column)\n",
    "                        if matn_id in keywords and keywords[matn_id] in line:\n",
    "                            results.append([matn_id, search_string, row['Matn'], line.strip(), os.path.abspath(txt_file)])\n",
    "\n",
    "output_df = pd.DataFrame(results, columns=['Matn_ID', 'SearchedString', 'FullText', 'LineFound', 'FilePath'])\n",
    "output_csv_path = 'Output3/File/Path/filename.csv'\n",
    "output_df.to_csv(output_csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Sort matches into a dataframe where for each Matn_ID, it shows in which books the Matn was found.\n",
    "\n",
    "# Create a list of unique \"Matn_ID\" values from CSV1 (file_A)\n",
    "\n",
    "unique_Matn_IDs = []\n",
    "\n",
    "with open(output_csv_path, 'r') as file_A:\n",
    "    csv_reader = csv.reader(file_A, delimiter=',')\n",
    "    next(csv_reader)  # Skip the header\n",
    "    for row in csv_reader:\n",
    "        Matn_ID = row[0]\n",
    "        if Matn_ID not in unique_Matn_IDs:\n",
    "            unique_Matn_IDs.append(Matn_ID)\n",
    "\n",
    "# Create a dictionary of \"Matn_ID\" to a list of associated \"reduced_path\" values\n",
    "Matn_ID_to_reduced_paths = {}\n",
    "with open(output_csv_path, 'r') as file_A:\n",
    "    csv_reader = csv.reader(file_A, delimiter=',')\n",
    "    next(csv_reader)  # Skip the header\n",
    "    for row in csv_reader:\n",
    "        Matn_ID = row[0]\n",
    "        FilePath = row[4]\n",
    "        reduced_path = FilePath.split('/')[-1].split('-')[0]\n",
    "\n",
    "        if Matn_ID in Matn_ID_to_reduced_paths:\n",
    "            Matn_ID_to_reduced_paths[Matn_ID].append(reduced_path)\n",
    "        else:\n",
    "            Matn_ID_to_reduced_paths[Matn_ID] = [reduced_path]\n",
    "\n",
    "# Initialize a dataframe with headers from CSV3 file and fill it based on the dictionary\n",
    "# Initialize a dataframe with headers from CSV2 file and fill it based on the dictionary\n",
    "headers = pd.read_csv('Dataframe/Path/with/MatnIDs/And/TextFileNames').columns[1:]\n",
    "sorted_output_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "# Add the \"Matn_ID\" column with unique \"Matn_ID\" values\n",
    "sorted_output_df.insert(0, \"Matn_ID\", unique_Matn_IDs)\n",
    "\n",
    "# Initialize all other columns to 0\n",
    "for col in headers:\n",
    "    sorted_output_df[col] = 0\n",
    "\n",
    "# Update the values in the dataframe based on the dictionary\n",
    "for Matn_ID, reduced_paths in Matn_ID_to_reduced_paths.items():\n",
    "    for reduced_path in reduced_paths:\n",
    "        #if sorted_output_df['Matn_ID'].isin([Matn_ID]).any():\n",
    "            #sorted_output_df.loc[sorted_output_df['Matn_ID'] == Matn_ID, reduced_path] += 1\n",
    "        if Matn_ID in sorted_output_df['Matn_ID'].values:\n",
    "            sorted_output_df.loc[sorted_output_df['Matn_ID'] == Matn_ID, reduced_path] += 1\n",
    "\n",
    "\n",
    "# Save the resulting dataframe to an output CSV file\n",
    "sorted_output_csv_path = 'Output4/File/Path/filename.csv'\n",
    "sorted_output_df.to_csv(sorted_output_csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e953247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Heatmap from Boolean values (Matn is either found or not).\n",
    "\n",
    "dat = pd.read_csv(sorted_output_csv_path, encoding='utf-8')\n",
    "\n",
    "# Select all columns except the first one which contains the UIDs\n",
    "dat2 = dat.iloc[:, 1:]\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "dat3 = dat2.to_numpy()\n",
    "\n",
    "# Convert all values greater than or equal to 1 to 1\n",
    "dat3[dat3 >= 1] = 1\n",
    "\n",
    "# Create an image plot (heatmap) of the data\n",
    "plt.figure()\n",
    "plt.imshow(dat3, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
